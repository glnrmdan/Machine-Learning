{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import latexify\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\displaystyle f(x) = \\sin x $$"
      ],
      "text/plain": [
       "<latexify.ipython_wrappers.LatexifiedFunction at 0x19151e35370>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@latexify.function\n",
    "def f(x):\n",
    "    return math.sin(x)\n",
    "\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\displaystyle \\mathrm{softmax}(x) = \\frac{\\exp x}{\\sum_{\\mathrm{xi} \\in x}^{} \\mathopen{}\\left({\\exp \\mathrm{xi}}\\mathclose{}\\right)} $$"
      ],
      "text/plain": [
       "<latexify.ipython_wrappers.LatexifiedFunction at 0x1915027c8e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax function\n",
    "@latexify.function\n",
    "def softmax(x):\n",
    "    return math.exp(x) / sum(math.exp(xi) for xi in x)\n",
    "\n",
    "softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\displaystyle \\mathrm{relu}(x) = \\mathrm{max} \\mathopen{}\\left( 0, x \\mathclose{}\\right) $$"
      ],
      "text/plain": [
       "<latexify.ipython_wrappers.LatexifiedFunction at 0x1915386d220>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ReLU function\n",
    "@latexify.function\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\displaystyle \\mathrm{neural\\_network}(x, w, b) = \\mathrm{relu} \\mathopen{}\\left( w x + b \\mathclose{}\\right) $$"
      ],
      "text/plain": [
       "<latexify.ipython_wrappers.LatexifiedFunction at 0x1915027c160>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Neural network with ReLU activation\n",
    "@latexify.function\n",
    "def neural_network(x, w, b):\n",
    "    return relu(w * x + b)\n",
    "\n",
    "neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\displaystyle \\mathrm{gradient\\_descent}(x, y, w, b, \\mathrm{alpha}) = \\mathopen{}\\left( w - \\mathrm{alpha} \\cdot 2 \\mathopen{}\\left( \\mathrm{neural\\_network} \\mathopen{}\\left( x, w, b \\mathclose{}\\right) - y \\mathclose{}\\right) x, b - \\mathrm{alpha} \\cdot 2 \\mathopen{}\\left( \\mathrm{neural\\_network} \\mathopen{}\\left( x, w, b \\mathclose{}\\right) - y \\mathclose{}\\right) \\mathclose{}\\right) $$"
      ],
      "text/plain": [
       "<latexify.ipython_wrappers.LatexifiedFunction at 0x19153480ee0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Descent\n",
    "@latexify.function\n",
    "def gradient_descent(x, y, w, b, alpha):\n",
    "    return (w - alpha * 2 * (neural_network(x, w, b) - y) * x,\n",
    "            b - alpha * 2 * (neural_network(x, w, b) - y))\n",
    "    \n",
    "gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\displaystyle \\begin{array}{l} g = 2 \\mathopen{}\\left( \\mathrm{neural\\_network} \\mathopen{}\\left( x, w, b \\mathclose{}\\right) - y \\mathclose{}\\right) x \\\\ m = \\mathrm{beta1} \\cdot m + \\mathopen{}\\left( 1 - \\mathrm{beta1} \\mathclose{}\\right) g \\\\ v = \\mathrm{beta2} \\cdot v + \\mathopen{}\\left( 1 - \\mathrm{beta2} \\mathclose{}\\right) g^{2} \\\\ \\mathrm{m\\_hat} = \\frac{m}{1 - \\mathrm{beta1}^{t}} \\\\ \\mathrm{v\\_hat} = \\frac{v}{1 - \\mathrm{beta2}^{t}} \\\\ w = w - \\frac{\\mathrm{alpha} \\cdot \\mathrm{m\\_hat}}{\\sqrt{ \\mathrm{v\\_hat} } + \\mathrm{epsilon}} \\\\ \\mathrm{adam}(x, y, w, b, \\mathrm{alpha}, \\mathrm{beta1}, \\mathrm{beta2}, \\mathrm{epsilon}, m, v, t) = \\mathopen{}\\left( w, b, m, v \\mathclose{}\\right) \\end{array} $$"
      ],
      "text/plain": [
       "<latexify.ipython_wrappers.LatexifiedFunction at 0x1915027ce20>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adam optimizer\n",
    "@latexify.function\n",
    "def adam(x, y, w, b, alpha, beta1, beta2, epsilon, m, v, t):\n",
    "    g = 2 * (neural_network(x, w, b) - y) * x\n",
    "    m = beta1 * m + (1 - beta1) * g\n",
    "    v = beta2 * v + (1 - beta2) * g**2\n",
    "    m_hat = m / (1 - beta1**t)\n",
    "    v_hat = v / (1 - beta2**t)\n",
    "    w = w - alpha * m_hat / (math.sqrt(v_hat) + epsilon)\n",
    "    return w, b, m, v\n",
    "\n",
    "adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
